{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeccb8e4-0101-47de-810f-60e4cb2c02c6",
   "metadata": {},
   "source": [
    "# Tester service\n",
    "\n",
    "FastAPI uses ASGI and asyncio to provide asynchronous server capabilities.\n",
    "\n",
    "Here, we do an analysis on how FastAPI is properly used and how it performs against a similar Flask setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143de33-d050-40b7-ae24-192d8f691d8e",
   "metadata": {},
   "source": [
    "We want to measure a few metrics:\n",
    "\n",
    "- Flask/Gunicorn vs FastAPI/Uvicorn on a control test with a CPU bound task (to ensure similar serverside execution times). Metrics : number of spawned processes and performance on N requests where N > workers-count.  \n",
    "- Flask vs FastAPI with IO bound tasks. Metrics : number of spawned processes and performance on N requests where N > workers-count.  \n",
    "- FastAPI bad use-cases vs proper async usage given a CPU bound task.  \n",
    "- Additionally we look at how FastAPI and Uvicorn spawn processes on the server machine under different usage contexts (ex: development with reload vs production with no reload).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf493352-6dea-47c8-8212-31e342d4360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e31e3b-6f73-462b-9eb6-7fc8e4352972",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84118e28-da9c-47ad-9f70-3526f42f0edc",
   "metadata": {},
   "source": [
    "## Tests design\n",
    "\n",
    "We use a Tester service and 3 Target Services (can be expanded to more cases):\n",
    "- Tester : Jupyter notebook with analysis.\n",
    "- Target-FastAPI:  FastAPI app served over Uvicorn app server configured as single worker.  \n",
    "- Target-FlaskSingle:  Flask app served over Gunicorn app server configured for 1xCPU cores.  \n",
    "- Target-FlaskDual:  Flask app served over Gunicorn app server configured for 2xCPU cores.  \n",
    "\n",
    "All services run under the same Docker network and are accessible via docker DNS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e335d8-b53e-43ce-be23-512e0c66733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Docker DNS for service addressing\n",
    "FASTAPI_SERVICE = \"fastapianalytics:5000\"\n",
    "FLASK_SINGLE_SERVICE = \"flask1xanalytics:5000\"\n",
    "FLASK_DUAL_SERVICE = \"flask2xanalytics:5000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78182ec7-81e2-4434-9f47-c41a698d4e15",
   "metadata": {},
   "source": [
    "## Test APIs\n",
    "\n",
    "All target services provide the same API urls for synchronous requests.  \n",
    "Only FastAPI target provides the async test apis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ecb57-9a12-4ed2-853c-d204aa866ba0",
   "metadata": {},
   "source": [
    "### Calibration step\n",
    "The first test is actually a calibration to server-machine resources: we run this to get an evaluation on how long the server needs for one calculation (varies with available CPU and memory, external processes, ..). \n",
    "\n",
    "### Baseline duration\n",
    "Based on the calibration value we can then determine the number of iterations required to have a specific duration for server operations.\n",
    "\n",
    "### Performance Test configuration\n",
    "Each test then sends a specific number of requests to a target, where each request is configured to have a specific duration for the serverside processing (i.e. each request takes X seconds to process serverside) .\n",
    "\n",
    "The tests are run in sequence, not parallel on the target servers. This is because we are using a single physical machine (services run in a Docker Network hosted on one machine) and services will compete for resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918c1725-c205-40d5-ae94-72dc62f7067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALIBRATION_API = \"test/calibrate\"\n",
    "SYNC_API_PERFORMANCE = \"test/sync-cpubound/{}\" # format the string and add specific duration for each test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c42db-5ed7-431b-818c-6d5a7da15fc8",
   "metadata": {},
   "source": [
    "Set up a convenience function to construct our urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc76ead2-557c-4236-bf46-6a4b3be067ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://fastapianalytics:5000/test/calibrate\n",
      "http://fastapianalytics:5000/test/sync-cpubound/2\n"
     ]
    }
   ],
   "source": [
    "def get_url(service, api, iterations=None):\n",
    "    partial = '/'.join(['http:/', service, api])\n",
    "    return partial.format(iterations) if iterations else partial\n",
    "# TESTS\n",
    "print(get_url(FASTAPI_SERVICE, CALIBRATION_API))\n",
    "print(get_url(FASTAPI_SERVICE, SYNC_API_PERFORMANCE, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12567c95-20f4-4db7-a0b7-f38dac183c36",
   "metadata": {},
   "source": [
    "# Calibration CPU bound tests\n",
    "\n",
    "Since response times may vary on host performance at a given moment, use an average over N requests to determine calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a0a8de-4b2e-4121-a068-9ff9b7922234",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVGSAMPLES_FOR_CALIB = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d98632-d7fd-4df4-a5fd-1fd21019a6b5",
   "metadata": {},
   "source": [
    "Prepare a convenience function to send http requests with or without iterations parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4988eb7c-f40b-4e5b-9427-a7c159013c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Duration of one iteration', 'result': 0.0886105999998108}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def send_http(url, iterations:int=0):\n",
    "    try:\n",
    "        \n",
    "        if iterations: # if there is a specific number of iterations, add it to the url\n",
    "            url = '/'.join([url, str(iterations)])\n",
    "        response = requests.request('GET', url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error sending request: {e}\")\n",
    "        raise e\n",
    "# TEST\n",
    "send_http(get_url(FASTAPI_SERVICE, CALIBRATION_API))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2fb08f-bb37-41d6-90d5-cd04f1d566bc",
   "metadata": {},
   "source": [
    "### Fast API Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6afa195d-8ec9-482d-8bb4-648d6514904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of 50 - duration on single iteration: 0.0348293459200795 seconds\n"
     ]
    }
   ],
   "source": [
    "fa_iter_d = sum([send_http(get_url(FASTAPI_SERVICE, CALIBRATION_API)).get('result') for _ in range(AVGSAMPLES_FOR_CALIB)])/AVGSAMPLES_FOR_CALIB\n",
    "print(f'Average of {AVGSAMPLES_FOR_CALIB} - duration on single iteration: {fa_iter_d} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879118e7-83b3-4382-88b1-731f3fe6f37a",
   "metadata": {},
   "source": [
    "### Flask targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eda9146-5adf-46de-8e52-d5a0167313e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of 50 - duration on single iteration: 0.02992334980008309 seconds\n"
     ]
    }
   ],
   "source": [
    "fsk1x_iter_d = sum([send_http(get_url(FLASK_SINGLE_SERVICE, CALIBRATION_API)).get('result') for _ in range(AVGSAMPLES_FOR_CALIB)])/AVGSAMPLES_FOR_CALIB\n",
    "print(f'Average of {AVGSAMPLES_FOR_CALIB} - duration on single iteration: {fsk1x_iter_d} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77dabba-330c-4d2e-8ed9-261729ad7886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of 50 - duration on single iteration: 0.030408311340288492 seconds\n"
     ]
    }
   ],
   "source": [
    "fsk2x_iter_d = sum([send_http(get_url(FLASK_DUAL_SERVICE, CALIBRATION_API)).get('result') for _ in range(AVGSAMPLES_FOR_CALIB)])/AVGSAMPLES_FOR_CALIB\n",
    "print(f'Average of {AVGSAMPLES_FOR_CALIB} - duration on single iteration: {fsk2x_iter_d} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017688ff-676f-4f5a-a135-cd94984225cb",
   "metadata": {},
   "source": [
    "### Iterations for N seconds processing time\n",
    "\n",
    "These do not reflect actual relative performance between services. The resources available for each service, when running the CPU bound task, may differ as do the actual, random values, in the test matrices.  \n",
    "The values only reflect a close approximation to how many iterations we need, on each service, to have a similarly timed computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b9d9c74-eb5c-4325-a9e5-30915d3717d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 66, 65)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2 # seconds\n",
    "faiter = int(N/fa_iter_d)\n",
    "fsk1xiter = int(N/fsk1x_iter_d)\n",
    "fsk2xiter = int(N/fsk2x_iter_d)\n",
    "faiter, fsk1xiter, fsk2xiter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a79781-707d-4fe1-8dd3-bc817919f6ed",
   "metadata": {},
   "source": [
    "# Base performance CPU bound - no parallelization\n",
    "\n",
    "This test will trigger **single requests** to synchronous apis on all targets.  \n",
    "These api-endpoints contain simple calls to CPU bound tasks, with no extra logic for parallelization.  \n",
    "Each request is parametrized to have the number of iterations required to induce a specific processing-time on the target server.  \n",
    "Here we establish a baseline for future tests where multiple requests are used to check for the target-server ability to handle concurrent requests.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bae4bd6-62d8-456b-be2b-50561a9b50e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Duration of 57 iterations', 'result': 1.6083624960010638}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa_cpu_base =send_http(get_url(FASTAPI_SERVICE, SYNC_API_PERFORMANCE, faiter))\n",
    "fa_cpu_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2abfc29b-4b9e-4e1f-a8c9-e66f89155397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Duration of 66 iterations', 'result': 1.8777725399995688}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsk1x_cpu_base =send_http(get_url(FLASK_SINGLE_SERVICE, SYNC_API_PERFORMANCE, fsk1xiter))\n",
    "fsk1x_cpu_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b8943b-4f7b-46d5-99c3-64f0b90b0d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Duration of 65 iterations', 'result': 1.929555138998694}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsk2x_cpu_base =send_http(get_url(FLASK_DUAL_SERVICE, SYNC_API_PERFORMANCE, fsk2xiter))\n",
    "fsk2x_cpu_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b2ccb-122f-4732-a723-df8a0abd53f5",
   "metadata": {},
   "source": [
    "We can already observe some pretty big differences between FastAPI and Flask performance: even though on a single iteration the FastAPI target was slower, when iterating multiple times the performance actually improved over both Flask Targets.  \n",
    "The relative performance between the 2 Flask Targets seems to be consistent to what we see on the single calibration measurements: tha one faster during calibration is still faster over multiple iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae56dd-5882-4110-aea1-b4555b6111f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
